---
title: "AirLines"
---
  
  ```
{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```






```{r}

library(ggplot2)
library(dplyr)
library(readr)

# Carica il dataset
data <- read.csv("Dataset/airline_passenger_satisfaction.csv") 

# Controlla i primi record
head(data)

```





## Second method: Feature selection with estimate value

Analysing the first model created above, the one with all the predictors(first logistic regression model implemented), it can be seen that most of the predictors are useful for calculating 'Satisfaction' outside the 'Distance_flight'.
I will now make a manual feature selection to see which predictors are the most significant in my model.

To select the best predictors, I analyse the results of the previous model and select the predictors with the highest estimated value. At each step, I add some predictors with a lower estimated value until I reach a satisfactory result.

For each model and then at each step, I add new predictors. Then I perform a series of tests to calculate the AUC area and finally calculate the accuracy.

### 00-estimate model

The first model uses the two predictors with the largest value: Customer.Type and Type.of.Travel.
In this case we take predictors that have the power of 00 as their estimate value.

```{r}
o0_model <- glm(Satisfaction ~ Customer.Type + Type.of.Travel, data = train, family = binomial)    

summary(o0_model)
```

Below the execution of the test set with the current model:
```{r}
o0_model_probs <- predict(o0_model, testX, type = "response")

o0_model_roc <- roc(test$Satisfaction ~ o0_model_probs, plot=TRUE, print.auc=TRUE)
```
Returns all information for the classification threshold of 0.5.
```{r}
coords(o0_model_roc, x=0.5, ret="all")
```
Returns all information for the classification usign the best threshold.
```{r}
coords(o0_model_roc, x="best", ret="all")
```

Check the collinearity between predictors in a regression model. Collinearity occurs when two or more predictors in a model are highly correlated.
```{r}
check_collinearity <- vif(o0_model)
check_collinearity
```

This first model shows that by using, as a basic criterion for assessing passenger satisfaction, the type of customer and the type of journey they make I can evaluate satisfaction in a good way.



### 01-estimate model
In the second model, we use the previous predictors and add others by taking estimate values with exponent 01.



```{r}
o1_model <- glm(Satisfaction ~ Customer.Type + Type.of.Travel + Class + Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + Check.in.Service + Online.Boarding + On.board.Service + Leg.Room.Service + Cleanliness + In.flight.Service + In.flight.Wifi.Service + In.flight.Wifi.Service + Baggage.Handling  
 , data = train, family = binomial)    

summary(o1_model)

```

Below the execution of the test set with the current model:
```{r}
o1_model_probs <- predict(o1_model, testX, type = "response")

o1_model_roc <- roc(test$Satisfaction ~ o1_model_probs, plot=TRUE, print.auc=TRUE)

```

Returns all information for the classification threshold of 0.5.
```{r}
coords(o1_model_roc, x=0.5, ret="all")
```
Returns all information for the classification usign the best threshold.
```{r}
coords(o1_model_roc, x="best", ret="all")
```

Check the collinearity between predictors in a regression model. Collinearity occurs when two or more predictors in a model are highly correlated.
```{r}
check_collinearity <- vif(o1_model)
check_collinearity
```


We can see that the second model brings clear improvements in the results but increasing the predictors from 2 to 13.





### 02-estimate model

We add further predictors to the model by taking values with exponent 02.

```{r}
o2_model <- glm(Satisfaction ~ Customer.Type + Type.of.Travel + Class + Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + Check.in.Service + Online.Boarding + On.board.Service + Leg.Room.Service + Cleanliness + In.flight.Service + In.flight.Wifi.Service + In.flight.Wifi.Service + Baggage.Handling + Gender + Seat.Comfort + In.flight.Entertainment 
 , data = train, family = binomial)    

summary(o2_model)

```
Below the execution of the test set with the current model:
```{r}
o2_model_probs <- predict(o2_model, testX, type = "response")

o2_model_roc <- roc(test$Satisfaction ~ o2_model_probs, plot=TRUE, print.auc=TRUE)
```

Returns all information for the classification threshold of 0.5.
```{r}
coords(o2_model_roc, x=0.5, ret="all")
```
Returns all information for the classification usign the best threshold.
```{r}
coords(o2_model_roc, x="best", ret="all")
```

Check the collinearity between predictors in a regression model. Collinearity occurs when two or more predictors in a model are highly correlated.
```{r}
check_collinearity <- vif(o2_model)
check_collinearity
```

In the third model we can see a slight improvement over the previous one, but the results are still very similar.
We increase from 13 to 16 predictors used.





### 03-estimate model

Last model, in this case we also use predictors with value 03.

```{r}
o3_model <- glm(Satisfaction ~ Customer.Type + Type.of.Travel + Class + Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + Check.in.Service + Online.Boarding + On.board.Service + Leg.Room.Service + Cleanliness + In.flight.Service + In.flight.Wifi.Service + In.flight.Wifi.Service + Baggage.Handling + Gender + Seat.Comfort + In.flight.Entertainment + Delay + Age
 , data = train, family = binomial)    

summary(o3_model)

```

Below the execution of the test set with the current model:
```{r}
o3_model_probs <- predict(o3_model, testX, type = "response")

o3_model_roc <- roc(test$Satisfaction ~ o3_model_probs, plot=TRUE, print.auc=TRUE)
```
Returns all information for the classification threshold of 0.5.
```{r}
coords(o3_model_roc, x=0.5, ret="all")
```
Returns all information for the classification usign the best threshold.
```{r}
coords(o3_model_roc, x="best", ret="all")
```

Check the collinearity between predictors in a regression model. Collinearity occurs when two or more predictors in a model are highly correlated.
```{r}
check_collinearity <- vif(o3_model)
check_collinearity
```
It can be seen that there is a strong collinearity between two predictors: Departure.Delay and Arrival.Delay.







## Summary of results of the second method

Below is a table summarising all the results of previous models:
```{r, echo = FALSE}
library(knitr)
library(ROCR)

model_data <- data.frame(
  Model = c("00-model", 
            "01-model", 
            "02-model", 
            "03-model"),
  AIC = c(AIC(o0_model),AIC(o1_model), AIC(o2_model), AIC(o3_model) ),
  AUC = c(auc(o0_model_roc),auc(o1_model_roc), auc(o2_model_roc),auc(o3_model_roc) ),
  `Threshold Accuracy 0.5` = c(0.77,0.87, 0.87, 0.87),
  `Best Threshold` = c(0.77, 0.87, 0.87, 0.87 ),
  `Number of predictors` = c(2,13,16,18),
  stringsAsFactors = FALSE
)


print(model_data)
```

We can see that the results are very similar except for the first model. Two predictors are not enough to have a complete model, but it is also not necessary to use all of them to achieve good results.





## Comparison results between the two method

The two models implemented are different in the way they are created. The first is implemented by evaluating individual predictors individually and then placing them in a class of their own. In turn, each class is used in a regression model to work out which type of service may be better for assessing the satisfaction of passers-by.
The second method instead selects the predictors on the basis of the estimate value calculated in the model 'model_with_all_predictors'. This method is based on adding predictors as the estimate value increases.
Although this is not a suitable method for carrying out such an analysis, I nevertheless wanted to include it for further comparison with the first method used.


AA first conclusion leads me to confirm that the model that performs best in terms of results and interpretability is the 'Best_model' model.
It allows me to understand which are the most important parameters to understand passenger satisfaction as previously mentioned.
In any case, the other models are also very good and slightly simpler; the results are still very similar.
In conclusion, passenger satisfaction can be predicted in different ways and based on different criteria: on airport services, on-board services, on the type of trip or on passenger comfort.

The best model groups these categories together to have a global evaluation but it is possible to use models with a smaller number of predictors based on some criteria compared to others.








## Lasso


```{r}
lasso_model<-glmnet(Satisfaction ~ ., data=train,family = "binomial", alpha = 1)
plot(lasso_model)

```



```{r}
set.seed(1)
cv.out <- cv.glmnet(Satisfaction ~ ., data=train,family = "binomial", alpha = 1, K=5)
cv.out
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
lasso.final <- glmnet(Satisfaction ~ .,  data=train , family = "binomial", alpha = 1, lambda = bestlam.lasso)
```



```{r}
plot(lasso_model, xvar = "lambda") 
abline(v = log(bestlam.lasso), lwd = 1.2, lty = "dashed")
```


The Lasso solution for the selected value of lambda is:
```{r}
coef(lasso_model, bestlam.lasso)
```




```{r}
lasso_model_probs <- predict(lasso_model, s = bestlam.lasso, newdata=test, type="response")
lasso_model_roc <- roc(test$Satisfaction ~ lasso_model_probs, plot=TRUE, print.auc=TRUE)
```


Roc function reports the following results with a threshold of 0.5:
```{r}
coords(lasso_model_roc, x=0.5, ret="all")
```
And the following considering the best threshold:
```{r}
coords(lasso_model_roc, x="best", ret="all")

```
We can also try to fit a logistic regression model choosing the predictors suggested by Lasso.

```{r}
lasso_log <- glm(Satisfaction ~ Age +Customer.Type + Type.of.Travel + Class + Flight.Distance + Departure.Delay + Arrival.Delay
+ Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + Check.in.Service + Online.Boarding + Gate.Location + On.board.Service + Seat.Comfort + Leg.Room.Service + Cleanliness +
 Food.and.Drink + In.flight.Service + In.flight.Wifi.Service + In.flight.Entertainment + Baggage.Handling  , data = train, family = binomial)
summary(lasso_log)
```






```{r}

```



















## Ridge Regression


```{r}
ridge_model<-glmnet(Satisfaction ~ ., data=train, family = "binomial", alpha = 0)
plot(ridge_model)
```





```{r}
set.seed(1)
cv.out <- cv.glmnet(Satisfaction ~ ., data=train,family = "binomial", alpha = 0, K=5)
cv.out
plot(cv.out)
bestlam_ridge <- cv.out$lambda.min
ridge_final <- glmnet(Satisfaction ~ .,  data=train , family = "binomial", alpha = 0, lambda = bestlam_ridge)
```

```{r}
plot(ridge_model, xvar = "lambda") 
abline(v = log(bestlam_ridge), lwd = 1.2, lty = "dashed")
```


The Lasso solution for the selected value of lambda is:
```{r}
coef(ridge_model, bestlam_ridge)
```


```{r}
ridge_model_probs <- predict(ridge_model, s = bestlam_ridge, newdata=test, type="response")
ridge_model_roc <- roc(test$Satisfaction ~ ridge_model_probs, plot=TRUE, print.auc=TRUE)
```









```{r}
coefficients <- coef(ridge_model)
print(coefficients)
```


```{r}
# Convertire i coefficienti in un data frame per una facile manipolazione
coeff_df <- as.data.frame(as.matrix(coefficients))
colnames(coeff_df) <- "Coefficient"
coeff_df$Variable <- rownames(coeff_df)
coeff_df <- coeff_df[order(abs(coeff_df$Coefficient), decreasing = TRUE), ]

# Visualizzare le prime variabili piÃ¹ influenti
print(head(coeff_df, 15))

```















```{r}
o0_model <- glm(Satisfaction ~ Customer.Type + Type.of.Travel, data = train, family = binomial)    

summary(o0_model)
```

Below the execution of the test set with the current model:
```{r}
o0_model_probs <- predict(o0_model, testX, type = "response")

o0_model_roc <- roc(test$Satisfaction ~ o0_model_probs, plot=TRUE, print.auc=TRUE)
```
Returns all information for the classification threshold of 0.5.
```{r}
coords(o0_model_roc, x=0.5, ret="all")
```
Returns all information for the classification usign the best threshold.
```{r}
coords(o0_model_roc, x="best", ret="all")
```













```{r}


```