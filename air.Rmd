---
title: "AirLines"
---
  
  ```
{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```






```{r}

library(ggplot2)
library(dplyr)
library(readr)

# Carica il dataset
data <- read.csv("Dataset/airline_passenger_satisfaction.csv") 

# Controlla i primi record
head(data)

```




## Stepwise Regression models

In this section I am going to implement the stepwise regression model using three different modes: forward, backward and both.



```{r}
null_model <- glm(Satisfaction ~ 1, data = train, family = binomial)
full_model <- glm(Satisfaction ~ ., data = train, family = binomial)
summary(full_model)
```


### Forward



```{r}
stepwise_model_forward <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")
summary(stepwise_model_forward)
```
AIC is a measure of the relative entropy of a model, i.e. the amount of information lost when using the model to describe the data.
A lower AIC value indicates a better model.
During model selection, the AIC values of different models are compared. 
The model with the lowest AIC is considered the best of those compared.



Below i run a test using the test set of mine dataset to see the capacity of the model:
```{r}

stepwise_model_forward_probs <- predict(stepwise_model_forward, testX, type = "response")

stepwise_model_forward_roc <- roc(test$Satisfaction ~ stepwise_model_forward_probs, plot=TRUE, print.auc=TRUE)
```
```{r}
coords(stepwise_model_forward_roc, x=0.5, ret="all")
```
And the following considering the best threshold:
```{r}
coords(stepwise_model_forward_roc, x="best", ret="all")

```

### Backward


```{r}
stepwise_model_backward <- step(full_model, scope = list(lower = null_model, upper = full_model), direction = "backward")
summary(stepwise_model_backward)

```


```{r}
stepwise_model_backward_probs <- predict(stepwise_model_backward, testX, type = "response")

stepwise_model_backward_roc <- roc(test$Satisfaction ~ stepwise_model_backward_probs, plot=TRUE, print.auc=TRUE)
```
roc function reports the following results with a threshold of 0.5:
```{r}
coords(stepwise_model_backward_roc, x=0.5, ret="all")
```
And the following considering the best threshold:
```{r}
coords(stepwise_model_backward_roc, x="best", ret="all")

```

### Both


```{r}
stepwise_model_both <- step(full_model, scope = list(lower = null_model, upper = full_model), direction = "both")
summary(stepwise_model_both)

```

```{r}
stepwise_model_both_probs <- predict(stepwise_model_both, testX, type = "response")

stepwise_model_both_roc <- roc(test$Satisfaction ~ stepwise_model_both_probs, plot=TRUE, print.auc=TRUE)
```

roc function reports the following results with a threshold of 0.5:
```{r}
coords(stepwise_model_backward_roc, x=0.5, ret="all")
```

And the following considering the best threshold:
```{r}
coords(stepwise_model_backward_roc, x="best", ret="all")
```

## AIC comparison between models
```{r}
AIC(new_model)
AIC(stepwise_model_forward)
AIC(stepwise_model_backward)
AIC(stepwise_model_both)
```



## Lasso


```{r}
lasso_model<-glmnet(Satisfaction ~ ., data=train,family = "binomial", alpha = 1)
plot(lasso_model)

```



```{r}
set.seed(1)
cv.out <- cv.glmnet(Satisfaction ~ ., data=train,family = "binomial", alpha = 1, K=5)
cv.out
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
lasso.final <- glmnet(Satisfaction ~ .,  data=train , family = "binomial", alpha = 1, lambda = bestlam.lasso)
```



```{r}
plot(lasso_model, xvar = "lambda") 
abline(v = log(bestlam.lasso), lwd = 1.2, lty = "dashed")
```


The Lasso solution for the selected value of lambda is:
```{r}
coef(lasso_model, bestlam.lasso)
```




```{r}
lasso_model_probs <- predict(lasso_model, s = bestlam.lasso, newdata=test, type="response")
lasso_model_roc <- roc(test$Satisfaction ~ lasso_model_probs, plot=TRUE, print.auc=TRUE)
```


Roc function reports the following results with a threshold of 0.5:
```{r}
coords(lasso_model_roc, x=0.5, ret="all")
```
And the following considering the best threshold:
```{r}
coords(lasso_model_roc, x="best", ret="all")

```
We can also try to fit a logistic regression model choosing the predictors suggested by Lasso.

```{r}
lasso_log <- glm(Satisfaction ~ Age +Customer.Type + Type.of.Travel + Class + Flight.Distance + Departure.Delay + Arrival.Delay
+ Departure.and.Arrival.Time.Convenience + Ease.of.Online.Booking + Check.in.Service + Online.Boarding + Gate.Location + On.board.Service + Seat.Comfort + Leg.Room.Service + Cleanliness +
 Food.and.Drink + In.flight.Service + In.flight.Wifi.Service + In.flight.Entertainment + Baggage.Handling  , data = train, family = binomial)
summary(lasso_log)
```






```{r}

```



















## Ridge Regression


```{r}
ridge_model<-glmnet(Satisfaction ~ ., data=train, family = "binomial", alpha = 0)
plot(ridge_model)
```





```{r}
set.seed(1)
cv.out <- cv.glmnet(Satisfaction ~ ., data=train,family = "binomial", alpha = 0, K=5)
cv.out
plot(cv.out)
bestlam_ridge <- cv.out$lambda.min
ridge_final <- glmnet(Satisfaction ~ .,  data=train , family = "binomial", alpha = 0, lambda = bestlam_ridge)
```

```{r}
plot(ridge_model, xvar = "lambda") 
abline(v = log(bestlam_ridge), lwd = 1.2, lty = "dashed")
```


The Lasso solution for the selected value of lambda is:
```{r}
coef(ridge_model, bestlam_ridge)
```


```{r}
ridge_model_probs <- predict(ridge_model, s = bestlam_ridge, newdata=test, type="response")
ridge_model_roc <- roc(test$Satisfaction ~ ridge_model_probs, plot=TRUE, print.auc=TRUE)
```









```{r}
coefficients <- coef(ridge_model)
print(coefficients)
```


```{r}
# Convertire i coefficienti in un data frame per una facile manipolazione
coeff_df <- as.data.frame(as.matrix(coefficients))
colnames(coeff_df) <- "Coefficient"
coeff_df$Variable <- rownames(coeff_df)
coeff_df <- coeff_df[order(abs(coeff_df$Coefficient), decreasing = TRUE), ]

# Visualizzare le prime variabili piÃ¹ influenti
print(head(coeff_df, 15))

```



```{r}


```